{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tolulade-A/opensource-llm-chatbot-blenderbot/blob/main/Chatbot_with_Open_Source_LLM_%26_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzHXsusLxXU_"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This project aims to build a chatbot platform using Large Language Models. Tools used here include;\n",
        "\n",
        "\n",
        "\n",
        "*   Hugging face model hub (to see models)\n",
        "*   Python\n",
        "*   Transformers (the brain box for our LLM)\n",
        "*   Blenderbot by FacebookAI -LLM\n",
        "\n",
        "\n",
        "\n",
        "LLM Use cases;\n",
        "\n",
        "This helps in chosing the right large model for our application.\n",
        "\n",
        "\n",
        "1.   Text generation\n",
        "2.   Language translation\n",
        "3.   Question & Answering\n",
        "4.   Sentimental analysis\n",
        "5.   Named entity recognition\n",
        "\n",
        "Additional factors to consider while selecting a language model for your projects include;\n",
        "\n",
        "1.  Licencing (Opensource & commercially available etc)\n",
        "2.  Type of training dat\n",
        "3.  Model size\n",
        "4.  Performance & accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RnhdCC1YmF"
      },
      "source": [
        "**Transformers** and LLMs work together within an application to enable conversation.\n",
        "\n",
        "What does a transformer really do?\n",
        "\n",
        "1. **Input processing**: When a message is received in an app, say chatbot here, the input data is processed, broken down into tokens (smaller bits).\n",
        "2. **Context understanding**: Transformers helps send the tokens to the LLM since the LLM has been trained on a lot of data, it tries to understand your context.\n",
        "3. **Response output**: If the LLM understands the message (tokens), it generates a response based on its understanding.Transformer will send you a response from the LLM that you can read or understand.\n",
        "4. Repeat.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpp9QKe3mgS"
      },
      "source": [
        "**Step 1:** Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44XeJNOEeyQo",
        "outputId": "6aa89b1e-67fd-4d7e-dbc0-15147e58373f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98TCIgKn3unf"
      },
      "source": [
        "**Step 2:** Import dependencies from the transformer library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V_Gpjryr315m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr8nk_yS33q8"
      },
      "source": [
        "**Step 3:** Chose the model you want\n",
        "\n",
        "Hugging face has a hub for this;\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n",
        "Let's use \"facebook/blenderbot-400M-distill\". It's opensource and fast\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-6HmSESj4G_2"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/blenderbot-400M-distill\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y256r6Ny4bUy"
      },
      "source": [
        "**Step 4:** Fetch & Initialise the model tokeniser\n",
        "\n",
        "If we run this script for the first time, the host machine will download the model from Hugging Face API. But after running the code once, the script will not re-download the model and will instead reference the local installation.\n",
        "\n",
        "Two terms here; **model** and **tokenizer**.\n",
        "\n",
        "In this script, we initiate variables using two handy classes from the transformers library:\n",
        "\n",
        "`model` is an instance of the class `AutoModelForSeq2SeqLM`, which allows us to interact with our chosen language model.\n",
        "\n",
        "`tokenizer` is an instance of the class `AutoTokenizer`, which optimizes our input and passes it to the language model efficiently. It does so by converting our text input to \"tokens\", which is how the model interprets the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bwho_LIU4hbh"
      },
      "outputs": [],
      "source": [
        "# Load model (download on first run and reference local installation for consequent runs)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ7bYUDy6GsT"
      },
      "source": [
        "**Step 5:** Chabot Process\n",
        "\n",
        "To start chatting;\n",
        "\n",
        "Let's do these to have an effective conversation with our chatbot.\n",
        "\n",
        "Before interacting with our model, we need to **initialize an object** where we can **store** our conversation **history**.\n",
        "\n",
        "Thereafter, we'll do the following for each interaction with the model:\n",
        "\n",
        "1. Encode conversation history as a string\n",
        "2. Fetch prompt from user\n",
        "3. Tokenise (optimise) prompt\n",
        "4. Generate output from model using prompt and history\n",
        "5. Decode output\n",
        "6. Update conversation history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE2aXTT36iTS"
      },
      "source": [
        "**Step 5a:** Keeping track of conversation history\n",
        "\n",
        "The conversation history is important when interacting with a chatbot because the chatbot will also reference the previous conversations when generating output.\n",
        "\n",
        "For our simple implementation in Python, we may simply use a **list**. Per the Hugging Face implementation, we will use this list to store the conversation history as follows:\n",
        "\n",
        "`conversation_history`\n",
        "\n",
        "`>> [input_1, output_1, input_2, output_2, ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fZ4Ts9Tp7l5O"
      },
      "outputs": [],
      "source": [
        "conversation_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0YBjiy57lMr"
      },
      "source": [
        "**Step 5b:** Encoding the conversation history\n",
        "\n",
        "During each interaction, we **will pass our conversation history to the model** along **with our input so that it may also reference the previous conversation** when *generating the next answer*.\n",
        "\n",
        "The transformers library function we are using expects to receive the conversation history as a string, with each element separated by the newline character '\\n'. Thus, we create such a string.\n",
        "\n",
        "We'll use the join() method in Python to do exactly that. (Initially, our history_string will be an empty string, which is okay, and will grow as the conversation goes on)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KvxbieC18E7x",
        "outputId": "5856414e-d7b5-44c7-b7be-29034438a060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history_string = \"\\n\".join(conversation_history)\n",
        "history_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2UdJ2YQ8aF5"
      },
      "source": [
        "**Step 5c:** Fetch prompt from user\n",
        "\n",
        "Let's run an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zl8S3Qnz8kxj"
      },
      "outputs": [],
      "source": [
        "#input_text = \"hello, how are you doing ?\"\n",
        "input_text = \"I had a piece of chocolate cake for lunch, so it was not too bad. Do you like cake ?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac_CzWo8vjV"
      },
      "source": [
        "**Step 5d:** Tokenisation of User Prompt and Chat History\n",
        "\n",
        "Tokens in NLP are individual units or elements that text or sentences are divided into. Tokenisation or vectorisation is the process of converting tokens into numerical representations.\n",
        "\n",
        "In NLP tasks, we often use the `encode_plus` method from the tokeniser object to perform tokenisation and vectorisation. Let's encode our inputs (prompt & chat history) as tokens so that we may pass them to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_zr7V9O9Pp7",
        "outputId": "ff7bd7d4-1291-4eda-add6-beb0edd7e22b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 281,  562,  265, 2725,  306, 7764, 6141,  335, 5344,   19,  394,  312,\n",
              "          372,  368,  618,  810,   21,  946,  304,  398, 6141, 2453]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuCWIdmn-LDU"
      },
      "source": [
        "In doing so, we've now created a **Python dictionary** which contains special keywords that allow the model to properly reference its contents. To learn more about tokens and their associated pretrained vocabulary files, you can explore the pretrained_vocab_files_map attribute. This attribute provides a mapping of pretrained models to their corresponding vocabulary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "40DZ5d0b-Mbc"
      },
      "outputs": [],
      "source": [
        "#tokenizer.pretrained_vocab_files_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXr4tgwz9zQ0"
      },
      "source": [
        "**Step 5e:** Generate output from model\n",
        "\n",
        "Now that we have our inputs ready, both past and present inputs, we can pass them to the model and generate a response. According to the documentation, we can use the generate() function and pass the inputs as keyword arguments (kwargs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0PQFDo79hKd",
        "outputId": "15246b57-bcdd-425f-8370-1ce3b9ea1b46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   1,  281,  361,  398, 6141,   19,  373,  281,  476,  368,  265,  893,\n",
              "         1599,  306, 7764,   21,  714,  906,  306, 6141,  372,  312,   38,    2]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model.generate(**inputs)\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GzLVGaY-Gij"
      },
      "source": [
        "**Step 5f:** Decode output\n",
        "\n",
        "We may decode the output using `tokenizer.decode()`. This is know as \"detokenization\" or \"reconstruction\". It is the process of combining or merging individual tokens back into their original form, typically to reconstruct the original text or sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2TgXnLq0-3nA",
        "outputId": "f3edc4fa-ea3b-4e91-d129-b01fa38422b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I do like cake, but I'm not a big fan of chocolate. What kind of cake was it?\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoebnHERgnYU"
      },
      "source": [
        "Alright! We've successfully had an interaction with our chatbot! We've given it a prompt, and we received its response.\n",
        "\n",
        "Now, all that's left to do is to update our conversation history, so that we may pass it with the next iteration.\n",
        "\n",
        "**Step 5g**: Update Conversation History\n",
        "\n",
        "All we need to do here is add both the input and response to `conversation_history` in plaintext.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvQ6SEiWgnYW",
        "outputId": "1da07826-c354-43ba-9a55-e22876d73fd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I had a piece of chocolate cake for lunch, so it was not too bad. Do you like cake ?',\n",
              " \"I do like cake, but I'm not a big fan of chocolate. What kind of cake was it?\"]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_history.append(input_text)\n",
        "conversation_history.append(response)\n",
        "conversation_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Rga4Wx_iZu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc5Iz_YegnYW"
      },
      "source": [
        "**Step 6**: Repeat\n",
        "\n",
        "We have gone through all the steps of interacting with your chatbot. Now, we can put everything in a loop and run a whole conversation!\n",
        "\n",
        "Please stop loop! (simply for demo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjilBuVSgnYX"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Create conversation history string\n",
        "    history_string = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Get the input data from the user\n",
        "    input_text = input(\"> \")\n",
        "\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs)\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Add interaction to conversation history\n",
        "    conversation_history.append(input_text)\n",
        "    conversation_history.append(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGr4gwdqOiTx"
      },
      "source": [
        "## **Deploying Chatbot to Backend Server**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-fGSwMyTtgf"
      },
      "source": [
        "## **Flask APP on Colab**\n",
        "\n",
        "Let's expose the flask app to the internet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-nm83kvS-tC",
        "outputId": "e0101290-3fe5-4a15-83cf-5cc76146ffc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "#when using flask on colab, not needed on local IDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU37pZpVeTSp"
      },
      "source": [
        "**Setup and Installation of Ngrok**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I75GO4YaeU8W",
        "outputId": "f6c9e0ce-21a4-4fd9-c9e4-07ed9219b3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-08 22:42:06--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13856790 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz.1’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]  13.21M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-07-08 22:42:06 (176 MB/s) - ‘ngrok-stable-linux-amd64.tgz.1’ saved [13856790/13856790]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# install ngrok linux version using the following command or you can get the\n",
        "# latest version from its official website- https://dashboard.ngrok.com/get-started/setup\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-HlTTBhecd-",
        "outputId": "c4fe718c-3b82-422e-8dd5-9ef6a832a7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok\n"
          ]
        }
      ],
      "source": [
        "# extract the downloaded file using the following command\n",
        "\n",
        "!tar -xvf /content/ngrok-stable-linux-amd64.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71E1yzcHLWpL"
      },
      "source": [
        "**The next step is to get your AuthToken from ngrok using this link-** https://dashboard.ngrok.com/get-started/your-authtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcKMxXXQegrQ",
        "outputId": "525efa95-d938-408c-df5a-1e67e59ca5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# paste your AuthToken here and execute this command\n",
        "\n",
        "!./ngrok authtoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxueQ9vrexZb"
      },
      "source": [
        "**Chatbot APP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2hTEcFvOmqG",
        "outputId": "2fde9a82-5b3c-447c-ae96-c6f73b8d3994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://41c4-35-221-27-36.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:14] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:19] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:23] \"\u001b[31m\u001b[1mGET /chatbot HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:23] \"\u001b[31m\u001b[1mGET /chatbot HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:24] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 00:58:39] \"POST /chatbot HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, render_template\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "from flask_ngrok import run_with_ngrok #for colab use only, remove in local IDE\n",
        "\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app) #for colab use only, remove in local IDE\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "conversation_history = []\n",
        "\n",
        "@app.route('/chatbot', methods=['POST'])\n",
        "def handle_prompt():\n",
        "    #read prompt from https request body\n",
        "    data = request.get_data(as_text = True)\n",
        "    data = json.loads(data)\n",
        "\n",
        "    input_text = data['prompt'] # Get the input data from the user\n",
        "\n",
        "    # Create conversation history string\n",
        "    history_string = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Get the input data from the user #when testing prototype\n",
        "    #input_text = input(\"> \")\n",
        "\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs, max_length=60)\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Add interaction to conversation history\n",
        "    conversation_history.append(input_text)\n",
        "    conversation_history.append(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPuu2b6+/ywZPc4fZ+QtJdt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
